{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFQtpUAVXZFRpa308Oy9xt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zcappai/AI-Stock-Trading/blob/main/AI_Trading_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFN-iZLPzHq3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Pseudocode for Deep Q-Learning\n",
        "State = (current share price, number of shares, remaining balance)\n",
        "Actions = (buy, sell, hold)\n",
        "Reward = Profit/loss from buying/selling/holding (change in portfolio value)\n",
        "Goal = Maximise the change of the portfolio value (maximise profits)\n",
        "epsilon = probability of taking a random action (initial value = 0.1, but can start at 1 with epsilon decay)\n",
        "gamma = discount rate (0 <= gamma <= 1), gamma closer to 1 takes future rewards into account more strongly, but closer to 0 it is more focused on immediate rewards\n",
        "alpha = learning rate (step-size) (0 <= alpha <= 1), closer to 1 means that it learns quickly and closer to 0 means it learns slowly\n",
        "\n",
        "The state could also be tested with additional values from the environment. e.g. the change in the stock price from the previous state.\n",
        "\n",
        "-- Process --\n",
        "State of stock price => buy/sell/hold => profit/loss from action\n",
        "\n",
        "I shall start with training the model on the data of one company and expand to many companies\n",
        "\n",
        "For each episode (year):\n",
        "  Set the initial state\n",
        "  For each step (day) in the episode (year):\n",
        "    Choose an action from the learned action-value function Q (with epsilon probability of being random)\n",
        "    Take the action and observe the returned reward and new state\n",
        "    New Q-value = current Q-value + alpha [reward + gamma * maximum expected future reward − current Q-value]\n",
        "    current state = new state\n",
        "  Until the state is terminal (end of the year)\n",
        "\n",
        "Should I factor in the number of shares bought/sold?\n",
        "\n",
        "Different numbers of layers in the neural network need to be experimented with.\n",
        "Different number of units per layer as well.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.losses import Huber\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from collections import deque\n",
        "from enum import Enum\n",
        "import random\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "class Company(Enum):\n",
        "  Apple = \"AAPL\"\n",
        " \n",
        "class Action(Enum):\n",
        "  HOLD = 0\n",
        "  BUY = 1\n",
        "  SELL = 2\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, currentSharePrice, startingBalance, company):\n",
        "    self.modelName = datetime.now().strftime(\"D%d/%m/%Y_T%H/%M/%S_model\")\n",
        "    self.numOfActions = 3 # 0 = hold, 1 = buy, 2 = sell\n",
        "    self.epsilon = 0.1\n",
        "    self.gamma = 0.95 # focused on long term rewards\n",
        "    self.alpha = 0.001 # learns more slowly\n",
        "    self.initialState = {\n",
        "        \"currentSharePrice\": currentSharePrice,\n",
        "        \"numOfShares\": 0,\n",
        "        \"startingBalance\": startingBalance\n",
        "        }\n",
        "    self.currentSharePrice = currentSharePrice\n",
        "    self.currentShares = 0\n",
        "    self.balance = startingBalance\n",
        "    self.actionQuantity = 10 # buy/sell 10 shares at a time\n",
        "    self.experienceReplay = deque(maxlen=5000)\n",
        "    self.miniBatchSize = 30\n",
        "    self.updateWeights = 30\n",
        "    self.company = company\n",
        "    self.generate_models()\n",
        "\n",
        "  def get_stock_data(self):\n",
        "    stockData = pd.read_csv(\"/content/gdrive/MyDrive/StockData/{}.csv\".format(self.company))\n",
        "    closingPrices = list(stockData[\"Close\"])\n",
        "    pass\n",
        "\n",
        "  def generate_models(self):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, activation=\"relu\", input_dim=3))\n",
        "    model.add(Dense(200, activation=\"relu\"))\n",
        "    model.add(Dense(300, activation=\"relu\"))\n",
        "    model.add(Dense(200, activation=\"relu\"))\n",
        "    model.add(Dense(100, activation=\"relu\"))\n",
        "    model.add(Dense(self.numOfActions))\n",
        "    adam = Adam(learning_rate = self.alpha)\n",
        "    model.compile(optimizer = adam, loss=Huber(delta=1.35))\n",
        "    self.model = model\n",
        "    self.target_model = model\n",
        "\n",
        "  def train(self):\n",
        "    self.state = self.initialState\n",
        "    for i in range(100):\n",
        "      if i % self.updateWeights == 0:\n",
        "        self.equalise_weights()\n",
        "\n",
        "      print(self.experienceReplay)\n",
        "      state = np.array([[self.state[k] for k in self.state]])\n",
        "      qValue = self.model.predict(state)\n",
        "      if (np.random.uniform() < self.epsilon):\n",
        "        chosenAction = np.random.randint(0, self.numOfActions)\n",
        "      else:\n",
        "        chosenAction = np.argmax(qValue[0])\n",
        "\n",
        "      if (chosenAction == Action.BUY.value):\n",
        "        self.balance -= self.actionQuantity * self.currentSharePrice\n",
        "        self.currentShares += self.actionQuantity\n",
        "        newState = {\n",
        "            \"currentSharePrice\": self.currentSharePrice,\n",
        "            \"numOfShares\": state[0][1] + self.actionQuantity,\n",
        "            \"startingBalance\": self.balance\n",
        "        }\n",
        "        reward = self.actionQuantity * self.currentSharePrice\n",
        "      elif (chosenAction == Action.SELL.value):\n",
        "        self.balance += self.actionQuantity * self.currentSharePrice\n",
        "        self.currentShares -= self.actionQuantity\n",
        "        newState = {\n",
        "            \"currentSharePrice\": self.currentSharePrice,\n",
        "            \"numOfShares\": state[0][1] - self.actionQuantity,\n",
        "            \"startingBalance\": self.balance\n",
        "        }\n",
        "        reward = self.actionQuantity * self.currentSharePrice\n",
        "      else:\n",
        "        newState = self.state\n",
        "        reward = 0\n",
        "      \n",
        "      newStateList = np.array([[newState[k] for k in self.state]])\n",
        "      self.experienceReplay.append((state, chosenAction, reward, newStateList))\n",
        "\n",
        "      if (len(self.experienceReplay) < self.miniBatchSize):\n",
        "        continue\n",
        "\n",
        "      miniBatch = random.sample(self.experienceReplay, self.miniBatchSize)\n",
        "\n",
        "      for sample in miniBatch:\n",
        "        sampleState, sampleAction, sampleReward, sampleNewState = sample\n",
        "        stateQValues = self.model.predict(sampleState)\n",
        "\n",
        "        newStateQValue = self.target_model.predict(sampleNewState)\n",
        "        newStateMaxQValue = np.max(newStateQValue[0])\n",
        "        targetQValue = sampleReward + self.gamma * newStateMaxQValue\n",
        "\n",
        "        stateQValues[0][sampleAction] = targetQValue\n",
        "\n",
        "        self.model.fit(sampleState, stateQValues, epochs=1, batch_size=self.miniBatchSize, verbose=2)\n",
        "\n",
        "      self.state = newState\n",
        "\n",
        "  def equalise_weights(self):\n",
        "    weights = self.model.get_weights()\n",
        "    target_weights = self.target_model.get_weights()\n",
        "    self.target_model.set_weights(weights)\n",
        "\n",
        "# All monetary values are in Pound sterling (£)\n",
        "currentSharePrice = 10\n",
        "startingBalance = 1000\n",
        "trainingCompany = Company.Apple.value\n",
        "agent = Agent(currentSharePrice, startingBalance, trainingCompany)\n",
        "agent.train()"
      ],
      "metadata": {
        "id": "S0-Ze6B6zmFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}